{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4ed9fa",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Evaluation Metrics\n",
    "\n",
    "In machine learning, model evaluation is crucial to understand how well a model performs on unseen data. Different tasks require different evaluation metrics. Below is a list of commonly used evaluation metrics for **classification** and **regression** problems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Classification Metrics](#classification-metrics)\n",
    "   - [Accuracy](#accuracy)\n",
    "   - [Precision](#precision)\n",
    "   - [Recall](#recall)\n",
    "   - [F1 Score](#f1-score)\n",
    "   - [AUC-ROC](#auc-roc)\n",
    "   - [Confusion Matrix](#confusion-matrix)\n",
    "2. [Regression Metrics](#regression-metrics)\n",
    "   - [Mean Absolute Error (MAE)](#mean-absolute-error-mae)\n",
    "   - [Mean Squared Error (MSE)](#mean-squared-error-mse)\n",
    "   - [Root Mean Squared Error (RMSE)](#root-mean-squared-error-rmse)\n",
    "   - [R-squared (R²)](#r-squared-r²)\n",
    "   - [Adjusted R-squared](#adjusted-r-squared)\n",
    "\n",
    "---\n",
    "\n",
    "## Classification Metrics\n",
    "\n",
    "### Accuracy\n",
    "**Definition**: Accuracy is the ratio of correctly predicted instances to the total instances.\n",
    "\\[\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "- **TP (True Positives)**: Correct positive predictions\n",
    "- **TN (True Negatives)**: Correct negative predictions\n",
    "- **FP (False Positives)**: Incorrect positive predictions\n",
    "- **FN (False Negatives)**: Incorrect negative predictions\n",
    "\n",
    "**Use case**: Suitable for balanced datasets where classes are evenly distributed.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision\n",
    "**Definition**: Precision measures the proportion of true positives among all positive predictions.\n",
    "\\[\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "**Use case**: Important in cases where **false positives** are costly (e.g., spam detection).\n",
    "\n",
    "---\n",
    "\n",
    "### Recall (Sensitivity or True Positive Rate)\n",
    "**Definition**: Recall is the proportion of true positives that were correctly identified.\n",
    "\\[\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "**Use case**: Used when **false negatives** are critical (e.g., disease detection).\n",
    "**for more info https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc**\n",
    "\n",
    "---\n",
    "\n",
    "### F1 Score\n",
    "**Definition**: The F1 score is the harmonic mean of precision and recall, offering a balance between the two.\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\]\n",
    "**Use case**: Useful when there’s an uneven class distribution and a balance between precision and recall is needed.\n",
    "\n",
    "---\n",
    "\n",
    "### AUC-ROC (Area Under the Receiver Operating Characteristic Curve)\n",
    "**Definition**: The ROC curve plots the true positive rate (recall) against the false positive rate. The AUC (Area Under the Curve) measures the entire two-dimensional area underneath the curve.\n",
    "- **AUC = 1**: Perfect classifier\n",
    "- **AUC = 0.5**: No skill classifier (random guessing)\n",
    "\n",
    "**Use case**: Suitable for imbalanced datasets and understanding a model's ability to distinguish between classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix\n",
    "**Definition**: A confusion matrix is a table used to evaluate the performance of a classification algorithm. It provides insights into how well the model's predictions match the actual labels.\n",
    "```\n",
    "           Predicted Positive | Predicted Negative\n",
    "-------------------------------------------------\n",
    "Actual Positive  |       TP          |       FN\n",
    "Actual Negative  |       FP          |       TN\n",
    "```\n",
    "**Use case**: Useful for understanding the number of correct/incorrect classifications across each class.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Metrics\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "**Definition**: MAE is the average of the absolute differences between predicted values and actual values.\n",
    "\\[\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "\\]\n",
    "- \\( y_i \\): Actual value\n",
    "- \\( \\hat{y_i} \\): Predicted value\n",
    "- \\( n \\): Number of observations\n",
    "\n",
    "**Use case**: MAE is easy to interpret and gives an idea of the average error in the predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "**Definition**: MSE is the average of the squared differences between predicted values and actual values.\n",
    "\\[\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]\n",
    "**Use case**: Commonly used in regression. It penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "**Definition**: RMSE is the square root of the MSE, giving an error estimate in the same units as the target variable.\n",
    "\\[\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}\n",
    "\\]\n",
    "**Use case**: Like MSE but easier to interpret since it’s in the same units as the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### R-squared (R²)\n",
    "**Definition**: R² measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "\\]\n",
    "- \\( \\bar{y} \\): Mean of the actual values\n",
    "\n",
    "**Use case**: Gives an idea of how well the model explains the variance in the data. A value closer to 1 indicates a better fit.\n",
    "\n",
    "---\n",
    "\n",
    "### Adjusted R-squared\n",
    "**Definition**: Adjusted R² adjusts the R² value by accounting for the number of predictors in the model.\n",
    "\\[\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n - 1}{n - p - 1}\n",
    "\\]\n",
    "- \\( n \\): Number of data points\n",
    "- \\( p \\): Number of predictors\n",
    "\n",
    "**Use case**: More reliable than R² when comparing models with different numbers of predictors, as it penalizes adding irrelevant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c48924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
