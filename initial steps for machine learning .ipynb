{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94eaf820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74be08e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Machine Learning Model Imports\n",
    "\n",
    "### **Install Required Libraries**\n",
    "\n",
    "To implement various machine learning models, you need to install the following libraries. Use the command below to install all at once:\n",
    "\n",
    "```bash\n",
    "pip install scikit-learn tensorflow xgboost lightgbm\n",
    "```\n",
    "\n",
    "### **Depending on the Classifiers/Regressors You Want to Use, Import the Corresponding Libraries:**\n",
    "\n",
    "- **Random Forest (Classifier & Regressor):**\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "  ```\n",
    "\n",
    "- **Decision Tree (Classifier & Regressor):**\n",
    "  ```python\n",
    "  from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "  ```\n",
    "\n",
    "\n",
    "- **SVM (Support Vector Machine) (Classifier & Regressor):**\n",
    "  ```python\n",
    "  from sklearn.svm import SVC, SVR\n",
    "  ```\n",
    "\n",
    "- **Linear Regression:**\n",
    "  ```python\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  ```\n",
    "\n",
    "- **Polynomial Regression:**\n",
    "  ```python\n",
    "  from sklearn.preprocessing import PolynomialFeatures\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  ```\n",
    "\n",
    "- **XGBoost (Classifier & Regressor):**\n",
    "  ```python\n",
    "  from xgboost import XGBClassifier, XGBRegressor\n",
    "  ```\n",
    "\n",
    "- **LightGBM (Classifier & Regressor):**\n",
    "  ```python\n",
    "  import lightgbm as lgb\n",
    "  ```\n",
    "\n",
    "### **Summary of Libraries**\n",
    "\n",
    "| Model Type         | Library Import                                         |\n",
    "|--------------------|--------------------------------------------------------|\n",
    "| Random Forest       | `from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor` |\n",
    "| Decision Tree       | `from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor`     |\n",
    "| LSTM                | `from tensorflow.keras.models import Sequential`<br>`from tensorflow.keras.layers import LSTM, Dense` |\n",
    "| SVM                 | `from sklearn.svm import SVC, SVR`                                      |\n",
    "| Linear Regression   | `from sklearn.linear_model import LinearRegression`                      |\n",
    "| Polynomial Regression | `from sklearn.preprocessing import PolynomialFeatures`<br>`from sklearn.linear_model import LinearRegression` |\n",
    "| XGBoost             | `from xgboost import XGBClassifier, XGBRegressor`                        |\n",
    "| LightGBM            | `import lightgbm as lgb`                                                |\n",
    "\n",
    "Make sure to choose the appropriate library imports based on the model you're working with!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c5d97",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1: Import the Data and Identify the Problem Type\n",
    "\n",
    "The first step in any machine learning task is to import your dataset and determine whether the problem is **Classification** or **Regression**.\n",
    "\n",
    "### **Steps to Follow:**\n",
    "\n",
    "1. **Import the Dataset:**\n",
    "   Use a suitable library like `pandas` to load your data. The dataset typically contains features and a target variable that you want to predict.\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   # Load your dataset\n",
    "   data = pd.read_csv('your_dataset.csv')\n",
    "   ```\n",
    "\n",
    "2. **Identify the Target Variable:**\n",
    "   Determine which column in the dataset is the **target variable** (i.e., the variable you're predicting). For instance:\n",
    "   \n",
    "   ```python\n",
    "   target_variable = data['target_column']\n",
    "   ```\n",
    "\n",
    "3. **Check the Type of the Problem:**\n",
    "   - **Classification Problem:** If the target variable contains **natural numbers** (i.e., discrete values or categories).\n",
    "   - **Regression Problem:** If the target variable contains **real numbers** (i.e., continuous values).\n",
    "\n",
    "   You can check this by analyzing the data type of the target variable or inspecting its values:\n",
    "   \n",
    "   ```python\n",
    "   # Check if target variable contains natural numbers (Classification) or real numbers (Regression)\n",
    "   if pd.api.types.is_integer_dtype(target_variable):\n",
    "       print(\"The problem is a Classification problem.\")\n",
    "   elif pd.api.types.is_float_dtype(target_variable):\n",
    "       print(\"The problem is a Regression problem.\")\n",
    "   else:\n",
    "       print(\"Unknown problem type. Please check the target variable.\")\n",
    "   ```\n",
    "\n",
    "### **Classification vs. Regression Summary:**\n",
    "\n",
    "- **Classification:** The target variable contains **discrete categories** or **classes**. Examples include predicting:\n",
    "  - Whether a customer will **buy** a product (yes/no).\n",
    "  - Which category a product belongs to (e.g., **A, B, C**).\n",
    "\n",
    "- **Regression:** The target variable contains **continuous real values**. Examples include predicting:\n",
    "  - The **price** of a house.\n",
    "  - A person’s **age**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84d4ac",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: Handling Categorical Data\n",
    "\n",
    "Before performing Exploratory Data Analysis (EDA) and data preprocessing, it's essential to check if the data contains categorical features. Machine learning models can't directly handle categorical data, so it needs to be converted into numerical form. There are various methods to make categorical data understandable for the machine learning models:\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Handling Categorical Data is Important:**\n",
    "1. **Models Require Numerical Input:** Most machine learning models require the data to be in numerical format. Converting categorical data ensures models can interpret it.\n",
    "2. **Preserves the Nature of Categories:** Proper encoding methods ensure that the relationship between categories is preserved or handled appropriately for the given problem.\n",
    "3. **Improves Model Performance:** Ensuring categorical data is encoded correctly can significantly affect the performance of your model, especially for tree-based models, neural networks, and linear models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Methods to Convert Categorical Data into Machine-Understandable Format:**\n",
    "\n",
    "#### **1. Label Encoding:**\n",
    "   Label encoding assigns a unique integer to each category. It is primarily used when the categorical variable has an inherent order (e.g., low, medium, high).\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "   # Instantiate LabelEncoder\n",
    "   label_encoder = LabelEncoder()\n",
    "\n",
    "   # Apply to categorical columns\n",
    "   data['category_column'] = label_encoder.fit_transform(data['category_column'])\n",
    "   ```\n",
    "\n",
    "   **When to Use:**\n",
    "   - When the categorical variable is **ordinal** (e.g., \"Low\", \"Medium\", \"High\").\n",
    "   - For variables with few categories.\n",
    "\n",
    "   **Pros:**\n",
    "   - Simple and quick.\n",
    "   - Suitable for ordinal data.\n",
    "\n",
    "   **Cons:**\n",
    "   - May introduce unintended relationships if used for non-ordinal categorical data (e.g., \"Red\", \"Blue\", \"Green\").\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. One-Hot Encoding:**\n",
    "   One-hot encoding creates a new binary column for each category of a categorical feature. This method is effective for **nominal** categorical data (categories without an inherent order).\n",
    "\n",
    "   ```python\n",
    "   # One-Hot Encoding using pandas get_dummies\n",
    "   encoded_data = pd.get_dummies(data, columns=['category_column'])\n",
    "   ```\n",
    "\n",
    "   **When to Use:**\n",
    "   - For **nominal** categorical data (e.g., colors, country names).\n",
    "   - When categories do not have a meaningful order.\n",
    "\n",
    "   **Pros:**\n",
    "   - Preserves the independence of each category.\n",
    "   - Suitable for non-ordinal data.\n",
    "\n",
    "   **Cons:**\n",
    "   - Can increase dimensionality, especially for features with many categories.\n",
    "   - Not efficient for high-cardinality categorical features.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Target Encoding:**\n",
    "   Target encoding replaces the categorical values with the average target value for each category. This method is sometimes used when the number of categories is large.\n",
    "\n",
    "   ```python\n",
    "   # Using mean of the target variable for each category\n",
    "   mean_target = data.groupby('category_column')['target'].mean()\n",
    "   data['category_encoded'] = data['category_column'].map(mean_target)\n",
    "   ```\n",
    "\n",
    "   **When to Use:**\n",
    "   - When categories have a strong relationship with the target variable.\n",
    "   - To reduce dimensionality for categorical features with many unique values.\n",
    "\n",
    "   **Pros:**\n",
    "   - Can improve performance when there is a strong relationship between the category and the target.\n",
    "   - Reduces high cardinality issues.\n",
    "\n",
    "   **Cons:**\n",
    "   - Prone to overfitting if not handled carefully.\n",
    "   - May leak information from the target into the features.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Choosing the Right Encoding is Crucial:**\n",
    "   - The choice of encoding method depends on the nature of the categorical feature (ordinal or nominal) and the number of unique categories.\n",
    "   - Incorrect encoding can introduce unintended relationships between categories (e.g., label encoding on non-ordinal data) or cause dimensionality issues (e.g., one-hot encoding with too many categories).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78992004",
   "metadata": {},
   "source": [
    "\n",
    "# Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "### **Why EDA is Important:**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a crucial step in any machine learning project as it helps to:\n",
    "1. **Understand the Structure and Nature of the Data:** Before jumping into model building, it is essential to understand the data's structure, types, and distribution.\n",
    "2. **Identify Missing Data:** Missing values can significantly affect a model's performance, so identifying and handling them early is necessary.\n",
    "3. **Detect Outliers and Anomalies:** Outliers can distort the performance of models, especially those sensitive to data scales, like linear models.\n",
    "4. **Discover Patterns and Relationships in Data:** EDA helps to visualize relationships between features and the target variable, revealing trends or patterns that may not be immediately obvious.\n",
    "5. **Assess Feature Importance and Multicollinearity:** EDA helps you understand which features are strongly related to the target and whether certain features are redundant.\n",
    "6. **Determine the Need for Preprocessing:** Based on EDA, you can decide whether the data needs transformations like normalization, standardization, or encoding for better model performance.\n",
    "\n",
    "### **Steps to Perform EDA:**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Understanding the Data Structure:**\n",
    "   Begin by loading the dataset and getting a general overview of its structure.\n",
    "\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "\n",
    "   # Load the dataset\n",
    "   data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "   # View the first few rows of the dataset\n",
    "   data.head()\n",
    "\n",
    "   # Summary statistics for numerical columns\n",
    "   data.describe()\n",
    "\n",
    "   # Check data types and for missing values\n",
    "   data.info()\n",
    "   ```\n",
    "\n",
    "   **Goal:** To get an initial understanding of the dataset, its size, data types, and potential missing values.\n",
    "\n",
    "   **Reason:** Knowing the structure of your data (e.g., types of variables, missing values, and initial statistical summaries) sets the foundation for further analysis and preprocessing.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Check for Missing Data:**\n",
    "   Missing data can cause models to behave unexpectedly, so it is critical to handle it before modeling.\n",
    "\n",
    "   ```python\n",
    "   # Check for missing values in each column\n",
    "   missing_values = data.isnull().sum()\n",
    "\n",
    "   print(missing_values)\n",
    "   ```\n",
    "\n",
    "   **Data Preprocessing Options:**\n",
    "   - **Impute missing values** using techniques like mean, median, or mode imputation.\n",
    "   - **Drop columns or rows** with excessive missing data.\n",
    "\n",
    "   **Reason:** Missing data can reduce the accuracy and reliability of machine learning models if left untreated. Detecting and dealing with missing values at the start ensures the integrity of the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Data Distribution:**\n",
    "   Use histograms to visualize the distribution of numerical features. This helps you identify if any features have skewed distributions that might need transformations.\n",
    "\n",
    "   ```python\n",
    "   import matplotlib.pyplot as plt\n",
    "\n",
    "   # Plot the distribution of numerical features\n",
    "   data.hist(bins=30, figsize=(15, 10))\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   **Goal:** Understand how the values of each feature are spread across the dataset. If you find skewed distributions, transformations like log scaling or Box-Cox can help normalize the data.\n",
    "\n",
    "   **Reason:** Understanding the distribution of features helps decide if they require transformations, such as normalizing skewed features for algorithms that assume normally distributed data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Handling Outliers:**\n",
    "   Outliers can skew the results of certain models. The **Interquartile Range (IQR)** method is a common way to detect and handle outliers.\n",
    "\n",
    "   #### **4.1. Detecting Outliers Using the IQR Method:**\n",
    "   ```python\n",
    "   Q1 = data.quantile(0.25)\n",
    "   Q3 = data.quantile(0.75)\n",
    "   IQR = Q3 - Q1\n",
    "\n",
    "   # Define outliers as points that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR\n",
    "   outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()\n",
    "\n",
    "   print(\"Outliers detected:\\n\", outliers)\n",
    "   ```\n",
    "\n",
    "   #### **4.2. Visualizing Outliers Using Box Plots:**\n",
    "   Box plots help visualize outliers in numerical features.\n",
    "\n",
    "   ```python\n",
    "   import seaborn as sns\n",
    "\n",
    "   # Boxplot for numerical features\n",
    "   plt.figure(figsize=(10, 6))\n",
    "   sns.boxplot(data=data)\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   **Reason:** Outliers can distort the predictions of certain models, particularly regression-based ones. Detecting and addressing outliers ensures more reliable and robust models.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Correlation Analysis:**\n",
    "   Correlation measures the strength and direction of the relationship between two numerical features. It is useful for detecting multicollinearity and identifying which features are highly related to the target variable.\n",
    "\n",
    "   #### **5.1. Correlation Matrix:**\n",
    "   ```python\n",
    "   # Correlation matrix\n",
    "   corr_matrix = data.corr()\n",
    "\n",
    "   # Plot the correlation matrix using a heatmap\n",
    "   sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   **Reason:** Understanding feature relationships helps identify multicollinearity (redundant features) and informs decisions about feature selection. Strong correlations with the target variable can indicate important predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Covariance Analysis:**\n",
    "   Covariance provides insight into how two features vary together, although it doesn’t standardize the relationship (unlike correlation).\n",
    "\n",
    "   ```python\n",
    "   # Covariance matrix\n",
    "   cov_matrix = data.cov()\n",
    "\n",
    "   print(cov_matrix)\n",
    "   ```\n",
    "\n",
    "   **Reason:** While correlation shows the strength and direction of the relationship, covariance helps to further assess how variables move in relation to each other. It is useful for understanding the data dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Categorical Data Analysis:**\n",
    "   Analyze the distribution of categorical variables to decide if they need encoding or other preprocessing.\n",
    "\n",
    "   ```python\n",
    "   # Check unique values in categorical columns\n",
    "   for col in data.select_dtypes(include=['object', 'category']).columns:\n",
    "       print(f\"{col}: {data[col].value_counts()}\")\n",
    "   ```\n",
    "\n",
    "   **Data Preprocessing Options:**\n",
    "   - **One-hot encoding** for nominal categories.\n",
    "   - **Label encoding** for ordinal categories.\n",
    "\n",
    "   **Reason:** Handling categorical data appropriately is important for feeding it into machine learning algorithms, particularly for decision trees, logistic regression, and neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Feature-Target Relationships:**\n",
    "   Understanding the relationship between features and the target variable is essential for feature selection and engineering.\n",
    "\n",
    "   ```python\n",
    "   # Scatter plot for numerical features vs target\n",
    "   sns.pairplot(data, hue='target_column')  # Replace 'target_column' with your actual target variable\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "   **Reason:** Identifying the relationship between features and the target variable helps in understanding which features are predictive and how they should be processed. For example, if the relationship is nonlinear, transformations may be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Determine if Normalization or Standardization is Required:**\n",
    "   \n",
    "   #### **9.1. Normalization:**\n",
    "   Normalization scales data to a range between 0 and 1, often useful when the magnitude of the values is important, such as in k-NN or Neural Networks.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "   scaler = MinMaxScaler()\n",
    "   normalized_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "   #### **9.2. Standardization:**\n",
    "   Standardization transforms data to have a mean of 0 and a standard deviation of 1. It’s useful for models like SVM or Logistic Regression, which assume normally distributed data.\n",
    "\n",
    "   ```python\n",
    "   from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   scaler = StandardScaler()\n",
    "   standardized_data = scaler.fit_transform(data)\n",
    "   ```\n",
    "\n",
    "   **Reason:** Different machine learning models have different expectations about the data scale. Understanding whether normalization or standardization is required ensures the data fits these assumptions and improves model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "\n",
    "Through **Exploratory Data Analysis (EDA)**, you can identify key characteristics of the data, understand if and how it requires preprocessing, and prepare it for modeling. EDA ensures that potential issues (missing data, outliers, irrelevant features, and improper scaling) are addressed before proceeding to the modeling phase.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c941944",
   "metadata": {},
   "source": [
    "## Model choice \n",
    "lets talk about some models in detail there hyper parametrs data distribution they work the best on why they came into existance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e818c17f",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning Evaluation Metrics\n",
    "\n",
    "In machine learning, model evaluation is crucial to understand how well a model performs on unseen data. Different tasks require different evaluation metrics. Below is a list of commonly used evaluation metrics for **classification** and **regression** problems.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Classification Metrics](#classification-metrics)\n",
    "   - [Accuracy](#accuracy)\n",
    "   - [Precision](#precision)\n",
    "   - [Recall](#recall)\n",
    "   - [F1 Score](#f1-score)\n",
    "   - [AUC-ROC](#auc-roc)\n",
    "   - [Confusion Matrix](#confusion-matrix)\n",
    "2. [Regression Metrics](#regression-metrics)\n",
    "   - [Mean Absolute Error (MAE)](#mean-absolute-error-mae)\n",
    "   - [Mean Squared Error (MSE)](#mean-squared-error-mse)\n",
    "   - [Root Mean Squared Error (RMSE)](#root-mean-squared-error-rmse)\n",
    "   - [R-squared (R²)](#r-squared-r²)\n",
    "   - [Adjusted R-squared](#adjusted-r-squared)\n",
    "\n",
    "---\n",
    "\n",
    "## Classification Metrics\n",
    "\n",
    "### Accuracy\n",
    "**Definition**: Accuracy is the ratio of correctly predicted instances to the total instances.\n",
    "\\[\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "- **TP (True Positives)**: Correct positive predictions\n",
    "- **TN (True Negatives)**: Correct negative predictions\n",
    "- **FP (False Positives)**: Incorrect positive predictions\n",
    "- **FN (False Negatives)**: Incorrect negative predictions\n",
    "\n",
    "**Use case**: Suitable for balanced datasets where classes are evenly distributed.\n",
    "\n",
    "---\n",
    "\n",
    "### Precision\n",
    "**Definition**: Precision measures the proportion of true positives among all positive predictions.\n",
    "\\[\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "**Use case**: Important in cases where **false positives** are costly (e.g., spam detection).\n",
    "\n",
    "---\n",
    "\n",
    "### Recall (Sensitivity or True Positive Rate)\n",
    "**Definition**: Recall is the proportion of true positives that were correctly identified.\n",
    "\\[\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "**Use case**: Used when **false negatives** are critical (e.g., disease detection).\n",
    "**for more info https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc**\n",
    "\n",
    "---\n",
    "\n",
    "### F1 Score\n",
    "**Definition**: The F1 score is the harmonic mean of precision and recall, offering a balance between the two.\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "\\]\n",
    "**Use case**: Useful when there’s an uneven class distribution and a balance between precision and recall is needed.\n",
    "\n",
    "---\n",
    "\n",
    "### AUC-ROC (Area Under the Receiver Operating Characteristic Curve)\n",
    "**Definition**: The ROC curve plots the true positive rate (recall) against the false positive rate. The AUC (Area Under the Curve) measures the entire two-dimensional area underneath the curve.\n",
    "- **AUC = 1**: Perfect classifier\n",
    "- **AUC = 0.5**: No skill classifier (random guessing)\n",
    "\n",
    "**Use case**: Suitable for imbalanced datasets and understanding a model's ability to distinguish between classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Confusion Matrix\n",
    "**Definition**: A confusion matrix is a table used to evaluate the performance of a classification algorithm. It provides insights into how well the model's predictions match the actual labels.\n",
    "```\n",
    "           Predicted Positive | Predicted Negative\n",
    "-------------------------------------------------\n",
    "Actual Positive  |       TP          |       FN\n",
    "Actual Negative  |       FP          |       TN\n",
    "```\n",
    "**Use case**: Useful for understanding the number of correct/incorrect classifications across each class.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Metrics\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "**Definition**: MAE is the average of the absolute differences between predicted values and actual values.\n",
    "\\[\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|\n",
    "\\]\n",
    "- \\( y_i \\): Actual value\n",
    "- \\( \\hat{y_i} \\): Predicted value\n",
    "- \\( n \\): Number of observations\n",
    "\n",
    "**Use case**: MAE is easy to interpret and gives an idea of the average error in the predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "**Definition**: MSE is the average of the squared differences between predicted values and actual values.\n",
    "\\[\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "\\]\n",
    "**Use case**: Commonly used in regression. It penalizes larger errors more than smaller ones, making it sensitive to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "**Definition**: RMSE is the square root of the MSE, giving an error estimate in the same units as the target variable.\n",
    "\\[\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}\n",
    "\\]\n",
    "**Use case**: Like MSE but easier to interpret since it’s in the same units as the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### R-squared (R²)\n",
    "**Definition**: R² measures the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "\\]\n",
    "- \\( \\bar{y} \\): Mean of the actual values\n",
    "\n",
    "**Use case**: Gives an idea of how well the model explains the variance in the data. A value closer to 1 indicates a better fit.\n",
    "\n",
    "---\n",
    "\n",
    "### Adjusted R-squared\n",
    "**Definition**: Adjusted R² adjusts the R² value by accounting for the number of predictors in the model.\n",
    "\\[\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n - 1}{n - p - 1}\n",
    "\\]\n",
    "- \\( n \\): Number of data points\n",
    "- \\( p \\): Number of predictors\n",
    "\n",
    "**Use case**: More reliable than R² when comparing models with different numbers of predictors, as it penalizes adding irrelevant features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
